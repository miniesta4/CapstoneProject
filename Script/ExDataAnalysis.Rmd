---
title: "Capstone Project: Milestone Report"
author: "miniesta4"
date: "April 14, 2020"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
# v202004151813
knitr::opts_chunk$set(echo = FALSE, root.dir = "../")

library(tm)
library(wordcloud)
library(ngram)

source("./Utils.R")
```

## Introduction

The task is to do some exploratory data analysis. The basic goal is to develop 
an understanding of the various statistical properties of the data set so that
you can build a good prediction model down the road.

## Text files: reading and sampling

Three text files are provided:

```{r list}
fics <- list.files("../Data/en_US", full.names = TRUE)
fics
```

Character vectors obtained as result of reading are considered too large for
analysis.

```{r blogs, cache=TRUE}
docs <- lapply(fics, readLines, encoding = "UTF-8", skipNul = TRUE)
names(docs) <- c("blogs", "news", "twitter")

magnitudes <- calcula_mag(docs)
knitr::kable(magnitudes)

```

Sampling is applied to keep just 5% of lines.

```{r samples, cache=TRUE}
set.seed(20200401)
samples <- lapply(magnitudes[2, ], function(x) sample(x, .05 * x))
docs_sample <- mapply(`[`, docs, samples)

rm("docs")

magnitudes_s <- calcula_mag(docs_sample)
knitr::kable(magnitudes_s)

```

Finally, non ASCII characters are removed.

```{r ascii, cache=TRUE}

docs_sample_ascii <- lapply(docs_sample, iconv, "UTF-8", "ASCII", "")
magnitudes_s_a <- calcula_mag(docs_sample_ascii)
knitr::kable(magnitudes_s_a)

```

## Corpus object

The character vectors are loaded into a Corpus object. A Corpus is a collection 
of documents, in this case the three documents obtained after sampling.

```{r corpus}
docs_corpus <- VCorpus(VectorSource(docs_sample_ascii))
inspect(docs_corpus)

```

## Pre-processing

Some transformations are provided by the tm package to mainly clean the data.

```{r get}
getTransformations()

```

All of them but stemming (reducing every word to its root) are applied. 
Also, translation to lower case.

```{r docs}
docs_corpus <- tm_map(docs_corpus, content_transformer(tolower))
docs_corpus <- tm_map(docs_corpus, removeNumbers)
docs_corpus <- tm_map(docs_corpus, removeWords, stopwords("english"))
docs_corpus <- tm_map(docs_corpus, removePunctuation)
docs_corpus <- tm_map(docs_corpus, stripWhitespace)
#docs_corpus <- tm_map(docs_corpus, stemDocument)
inspect(docs_corpus)

```

## Document term matrix

A document term matrix (DTM) is created from the transformed corpus. The DTM is
a matrix that lists all occurrences of words in the corpus. Summary information
on the matrix follows.

```{r dtm}
dtm <- DocumentTermMatrix(docs_corpus)
dtm
```

Summary of cumulative frequencies of words across documents.

```{r freq}
freq <- colSums(as.matrix(dtm))
summary(freq)
hist(x = freq, main = "Histogram of Cumulative Frequencies of Words", 
     xlab = NULL)
```

Twenty most frequent occurring terms.

```{r ord}
ord <- order(freq, decreasing = TRUE)
freq[ord[1: 20]]
```

```{r find}
# findFreqTerms(dtm, lowfreq = 1000)
```

Word cloud with 100 most frequent terms.

```{r cloud}
wordcloud(names(freq), freq, scale = c(3, .5), max.words = 100)
```

## N-Grams

An n-gram is an ordered sequence of n “words” taken from a body of text. 
They are the base for a prediction model of next word. 
The ngram package allows for fast n-gram tokenization among other very useful utilities.

#### Bigrams

Sequences of two words.

```{r cadena}
cadena <- concatenate(lapply(docs_corpus, "[", 1))
cadena <- preprocess(cadena, case = "lower", remove.punct = TRUE, 
                     remove.numbers = TRUE, fix.spacing = TRUE)

n2_gram <- ngram(cadena, n = 2)
n2_gram_freqs <- get.phrasetable(n2_gram)
summary(n2_gram_freqs$freq)
hist(n2_gram_freqs$freq, main = "Histogram of Bigrams", xlab = NULL)
n2_gram_freqs[order(n2_gram_freqs$freq, decreasing = TRUE), ][1:10, ]
```

#### Trigrams

Sequencies of three words.

```{r n3}
n3_gram <- ngram(cadena, n = 3)
n3_gram_freqs <- get.phrasetable(n3_gram)
summary(n3_gram_freqs$freq)
hist(n3_gram_freqs$freq, main = "Histogram of Trigrams", xlab = NULL)
n3_gram_freqs[order(n3_gram_freqs$freq, decreasing = TRUE), ][1:10, ]
```


#### 4-grams

Sequencies of four words.

```{r n4}
n4_gram <- ngram(cadena, n = 4)
n4_gram_freqs <- get.phrasetable(n4_gram)
summary(n4_gram_freqs$freq)
hist(n4_gram_freqs$freq, main = "Histogram of 4-grams", xlab = NULL)
n4_gram_freqs[order(n4_gram_freqs$freq, decreasing = TRUE), ][1:10, ]
```

## Conclusions

* The documents provided for the project are too large. Some kind of sampling
will be applied in order to reduce the size of the dataset used to build the model.
* Transformations provided by the tm package are not enough to clean the text. 
Some manual cleaning through base functions will be needed. Removal of stopwords
will have to be evaluated as probably are not suitable for the prediction model.
* The ngram package provides some utilities and a tokenizer which are very well
suited to build the prediction model.
